# -*- coding: utf-8 -*-
"""pyspark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYI25k21oyF77R6PPFxqULCehJUJg1Vx

# Preparação do Ambiente Spark

# Instalação do Java 8
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Download e extração do Apache Spark
!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz
!tar xf spark-3.2.4-bin-hadoop3.2.tgz

# Instalação das bibliotecas findspark e pyspark
!pip install -q findspark
!pip install -q pyspark

# Configuração das variáveis de ambiente para o Spark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64/"
os.environ["SPARK_HOME"] = "/content/spark-3.2.4-bin-hadoop3.2/"

# Inicialização do Spark

# Importando e inicializando a biblioteca findspark
import findspark
findspark.init()

# Importação do SparkSession para criar uma sessão Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Carregamento de um conjunto de dados CSV
dataset = spark.read.csv('/content/sample_data/california_housing_test.csv', inferSchema=True, header=True)

# Exibição do esquema e das primeiras linhas do conjunto de dados
dataset.printSchema()
dataset.head()

# Criação de uma visualização temporária da tabela no Spark
dataset.createOrReplaceTempView("tabela_temporaria")
print(spark.catalog.listTables())

# Execução de uma consulta SQL no Spark
query = "FROM tabela_temporaria SELECT longitude, latitude LIMIT 3"
saida = spark.sql(query)
saida.show()

# Execução de outra consulta SQL para encontrar o máximo de quartos
query1 = "SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria"
q_maximo_quartos = spark.sql(query1)

# Conversão do resultado da consulta para um DataFrame Pandas
pd_maximo_quartos = q_maximo_quartos.toPandas()
print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos['maximo_quartos']))
qtd_maximo_quartos = int(pd_maximo_quartos.loc[0, 'maximo_quartos'])

# Convertendo Pandas DataFrame para Spark DataFrame

# Importando bibliotecas necessárias
import pandas as pd
import numpy as np

# Criando um DataFrame Pandas com dados aleatórios
media = 0
desvio_padrao = 0.1
pd_temporario = pd.DataFrame(np.random.normal(media, desvio_padrao, 100))

# Convertendo o DataFrame Pandas em um DataFrame Spark
spark_temporario = spark.createDataFrame(pd_temporario)
print(spark.catalog.listTables())

# Criando uma visualização temporária da nova tabela no Spark
spark_temporario.createOrReplaceTempView("nova_tabela_temporaria")
print(spark.catalog.listTables())

# Encerrando a sessão Spark
spark.stop()

# Práticas no PySpark - Transformação

# Importando SparkContext para iniciar um novo contexto Spark
from pyspark import SparkContext
spark_contexto = SparkContext()

# Importando a biblioteca numpy
import numpy as np

# Criando um vetor numpy
vetor = np.array([10, 20, 30, 40, 50])

# Paralelizando o vetor para criar um RDD Spark
paralelo = spark_contexto.parallelize(vetor)

# Exibindo o RDD paralelo
print(paralelo)

# Aplicando uma operação de mapeamento (transformação) ao RDD
mapa = paralelo.map(lambda x: x**2 + x)

# Coletando os resultados
mapa.collect()

# Exemplo 2

# Criando um novo RDD com uma lista de palavras
paralelo = spark_contexto.parallelize(["distribuida", "distribuida", "spark", "rdd", "spark", "spark"])

# Definindo uma função lambda para mapeamento
funcao_lambda = lambda x: (x, 1)

# Importando a função 'add' da biblioteca 'operator'
from operator import add

# Aplicando a função lambda e reduzindo por chave (reduceByKey)
mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()

# Exibindo os resultados
for (w, c) in mapa:
  print("{}: {}".format(w, c))

# Encerrando o contexto Spark
spark_contexto.stop()

# Práticas no PySpark - Ação

# Importando SparkContext para iniciar um novo contexto Spark
from pyspark import SparkContext
spark_contexto = SparkContext()

# Criando uma lista
lista = [1, 2, 3, 4, 5, 3]

# Paralelizando a lista para criar um RDD Spark
lista_rdd = spark_contexto.parallelize(lista)

# Contando o número de elementos no RDD
lista_rdd.count()

# Definindo uma função lambda para criar pares ordenados
par_ordenado = lambda numero: (numero, numero * 10)

# Aplicando a função lambda com flatMap e coletando os resultados
lista_rdd.flatMap(par_ordenado).collect()

# Aplicando a função lambda com map e coletando os resultados
lista_rdd.map(par_ordenado).collect()

# Encerrando o contexto Spark
spark_contexto.stop()
